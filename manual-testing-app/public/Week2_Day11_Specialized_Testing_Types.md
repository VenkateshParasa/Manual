# Day 11: Specialized Testing Types
## Comprehensive Study Guide

---

## Table of Contents
1. [Localization & Internationalization Testing](#localization--internationalization-testing)
2. [Accessibility Testing (WCAG Deep Dive)](#accessibility-testing-wcag-deep-dive)
3. [Usability Testing](#usability-testing)
4. [Compatibility Testing](#compatibility-testing)
5. [Practical Exercises](#practical-exercises)
6. [Assessment Questions](#assessment-questions)

---

## Localization & Internationalization Testing

### 1.1 Understanding i18n and L10n

**Definitions:**

```


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
INTERNATIONALIZATION (i18n) vs LOCALIZATION (L10n)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

INTERNATIONALIZATION (i18n)
"i" + 18 letters + "n"

Definition: Designing software so it CAN be adapted to various languages and regions WITHOUT code changes.

Key Activities:
âœ“ Externalizing strings (not hardcoded)
âœ“ Supporting Unicode (UTF-8)
âœ“ Using locale-aware functions
âœ“ Designing flexible UI layouts
âœ“ Separating code from content
âœ“ Supporting multiple character sets

Example:
// BAD (Not internationalized)
message = "Welcome, " + userName + "!";

// GOOD (Internationalized)
message = getMessage("welcome_message", userName);

When: During development (architecture decision)
Responsibility: Developers

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

LOCALIZATION (L10n)
"L" + 10 letters + "n"

Definition: Adapting software for a SPECIFIC language, region, or culture.

Key Activities:
âœ“ Translating text
âœ“ Adapting date/time formats
âœ“ Adapting currency formats
âœ“ Adapting number formats
âœ“ Using local images/colors
âœ“ Adjusting layout for text expansion
âœ“ Cultural adaptation

Example:
English: "Welcome, John!"
Spanish: "Â¡Bienvenido, John!"
Japanese: "ã‚ˆã†ã“ãã€Johnã•ã‚“!"
Arabic: "!John ØŒÙ…Ø±Ø­Ø¨Ø§" (RTL - Right-to-Left)

When: After development, per target market
Responsibility: Translators, localization team, QA

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

RELATIONSHIP:
i18n (once) â†’ L10n (many times)

Think of it as:
i18n = Building a house with removable wallpaper
L10n = Changing the wallpaper for each customer

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### 1.2 Language Translation Testing

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
TRANSLATION TESTING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

TC-L10N-001: Translation completeness
Steps:
1. Switch to target language (e.g., Spanish)
2. Navigate through all screens
3. Check every UI element

Expected:
âœ“ All text translated (no English remnants)
âœ“ Buttons, labels, messages in target language
âœ“ Error messages translated
âœ“ Help text translated
âœ— No "Lorem ipsum" placeholder text

Common Issues:
âœ— "Login" button not translated
âœ— Error message still in English
âœ— Hardcoded strings not externalized

TC-L10N-002: Translation accuracy
Prerequisite: Native speaker review
Steps: Review translations for correctness

Expected:
âœ“ Translations accurate (correct meaning)
âœ“ Context-appropriate (formal vs informal)
âœ“ Industry terminology correct
âœ“ No awkward phrasing

Example Issues:
âœ— Machine translation errors (Google Translate)
âœ— Wrong context (e.g., "Bank" = financial vs riverbank)
âœ— Overly literal translation

TC-L10N-003: Text expansion (truncation)
Context: Many languages expand compared to English

Expansion Examples:
English â†’ German: +30% average
English â†’ Spanish: +20-30%
English â†’ French: +15-20%
English â†’ Finnish: +30-40%

Test Case:
English: "Submit" (6 characters)
German: "Einreichen" (10 characters)
Spanish: "Enviar" (6 characters)
French: "Soumettre" (9 characters)

Steps:
1. Switch to language with expansion
2. Check all UI elements

Expected:
âœ“ Text fits in buttons (no truncation)
âœ“ Labels not cut off
âœ“ Dialogs resize appropriately
âœ— No text overflow

Design Tips:
âœ“ Allow 30-40% extra space for buttons
âœ“ Use responsive layouts
âœ“ Test with longest expected translation

TC-L10N-004: Text contraction
Context: Some languages are shorter

Examples:
English â†’ Chinese: -30% or more
English â†’ Japanese: -10-20%

Steps: Switch to shorter language, check layout

Expected:
âœ“ UI still looks good (not stretched)
âœ“ Appropriate spacing maintained

TC-L10N-005: Concatenated strings
Bad Practice Example:
English: message = "Hello, " + name + "!"
French: message = "Bonjour, " + name + "!"

Problem:
German: "Hallo, " + name + "!" (works)
Spanish: "Â¡Hola, " + name + "!" (Â¡ before name - doesn't work!)

Test:
âœ“ No broken sentences
âœ“ Grammar correct with variables

Solution: Use placeholders
English: "Hello, {name}!"
Spanish: "Â¡Hola, {name}!"

TC-L10N-006: Pluralization rules
English: Simple (1 item, 2+ items)
1 item
2 items

Other languages: More complex

Arabic: 6 plural forms
0, 1, 2, 3-10, 11-99, 100+

Russian: 3 forms
1, 2-4, 5+
1 Ñ„Ğ°Ğ¹Ğ» (1 file)
2 Ñ„Ğ°Ğ¹Ğ»Ğ° (2 files)
5 Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² (5 files)

Test Case:
Steps: Test with 0, 1, 2, 3, 5, 10, 100 items

Expected:
âœ“ Correct plural form for each number
âœ“ Grammar correct

TC-L10N-007: Special characters
Test characters unique to language:
- Spanish: Ã±, Ã¡, Ã©, Ã­, Ã³, Ãº, Ã¼, Â¿, Â¡
- German: Ã¤, Ã¶, Ã¼, ÃŸ
- French: Ã©, Ã¨, Ãª, Ã«, Ã , Ã¹, Ã§, Å“
- Portuguese: Ã£, Ãµ, Ã§
- Scandinavian: Ã¥, Ã¤, Ã¶
- Polish: Ä…, Ä‡, Ä™, Å‚, Å„, Ã³, Å›, Åº, Å¼

Expected:
âœ“ All special characters display correctly
âœ“ Search works with special characters
âœ“ Input accepts special characters
âœ“ Sorting works correctly (Ã¤ after a, Ã± after n)

TC-L10N-008: Encoding (UTF-8)
Test: Various languages in one app

Expected:
âœ“ All character sets display correctly
âœ“ No garbled text (ï¿½ï¿½ï¿½ï¿½)
âœ“ UTF-8 encoding used throughout
âœ“ Database stores Unicode correctly

TC-L10N-009: Language switcher
Steps:
1. Find language selection (settings, menu)
2. Switch language
3. Navigate app

Expected:
âœ“ Language changes immediately OR after restart
âœ“ All content updates to new language
âœ“ Selection persists

TC-L10N-010: Fallback language
Steps:
1. Request unsupported language
2. Observe behavior

Expected:
âœ“ Falls back to default (usually English)
âœ“ Partial translation if available
âœ“ No blank screens

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### 1.3 Cultural Adaptation Testing

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
CULTURAL ADAPTATION TESTING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

TC-CULTURE-001: Date format
Different formats worldwide:

US: MM/DD/YYYY (03/15/2026)
Europe: DD/MM/YYYY (15/03/2026)
Japan: YYYY/MM/DD (2026/03/15)
ISO: YYYY-MM-DD (2026-03-15)

Test:
âœ“ Correct format for user's locale
âœ“ Ambiguous dates avoided (use month names)
âœ“ Date picker uses local format

TC-CULTURE-002: Time format
12-hour (US): 3:00 PM
24-hour (Most of world): 15:00

Test:
âœ“ Correct format for locale
âœ“ AM/PM displayed if 12-hour
âœ“ Time picker matches locale

TC-CULTURE-003: Number format
Different decimal and thousand separators:

US: 1,234.56 (comma thousands, period decimal)
Europe: 1.234,56 (period thousands, comma decimal)
India: 1,23,456.78 (lakhs system)

Test:
âœ“ Correct separators for locale
âœ“ Input accepts local format
âœ“ Display uses local format

TC-CULTURE-004: Currency format
Different currency symbols and placement:

US: $1,234.56 (symbol before, comma thousands, period decimal)
Europe: 1.234,56 â‚¬ (symbol after, period thousands, comma decimal)
UK: Â£1,234.56
Japan: Â¥1,234
India: â‚¹1,23,456.78

Test:
âœ“ Correct currency symbol
âœ“ Correct format for locale
âœ“ Currency conversion accurate
âœ“ Symbol positioned correctly

TC-CULTURE-005: Phone number format
Different formats:

US: (555) 123-4567 or 555-123-4567
UK: 020 1234 5678
France: 01 23 45 67 89
Germany: 030 12345678
International: +1 555 123 4567

Test:
âœ“ Accepts local format
âœ“ Validates correctly
âœ“ Displays with appropriate formatting
âœ“ International format supported

TC-CULTURE-006: Address format
Different address structures:

US:
John Doe
123 Main Street
Apt 4B
New York, NY 10001
USA

UK:
John Doe
Flat 4B
123 High Street
London
SW1A 1AA
United Kingdom

Japan: (Reverse order)
ã€’100-0001
æ±äº¬éƒ½åƒä»£ç”°åŒºåƒä»£ç”°1-1
ç”°ä¸­å¤ªéƒ

Test:
âœ“ Address fields appropriate for country
âœ“ Required fields match local standards
âœ“ Postal code format validated correctly

TC-CULTURE-007: Name format
Different name structures:

Western: FirstName LastName (John Doe)
Chinese: LastName FirstName (ææ˜ = Li Ming)
Spanish: FirstName LastName MotherLastName (Juan GarcÃ­a LÃ³pez)
Single name: Madonna, Cher (mononym)

Test:
âœ“ Name field accepts various formats
âœ“ No "First Name" / "Last Name" assumptions
âœ“ Sorting works appropriately

TC-CULTURE-008: Colors and symbols
Cultural meanings vary:

Red:
- Western: Danger, stop, love
- China: Good luck, celebration
- South Africa: Mourning

White:
- Western: Purity, peace
- Eastern: Death, mourning

Symbols:
- Thumbs up ğŸ‘: Offensive in some Middle Eastern countries
- OK sign ğŸ‘Œ: Offensive in some countries

Test:
âœ“ Colors appropriate for target culture
âœ“ Symbols not offensive
âœ“ Images culturally appropriate

TC-CULTURE-009: Right-to-Left (RTL) languages
Languages: Arabic, Hebrew, Persian, Urdu

Layout changes:
- Text direction: right to left
- UI mirrored: back button on right, menu on left
- Numbers: Still left to right (123, not 321)
- Icons: Some mirror (back arrow), some don't (play button)

TC-RTL-001: RTL layout
Steps: Switch to Arabic or Hebrew

Expected:
âœ“ Text flows right to left
âœ“ UI elements mirrored horizontally
âœ“ Scrollbars on left side
âœ“ Navigation reversed (back button on right)
âœ“ Alignment: Right-aligned instead of left

TC-RTL-002: Mixed content (LTR in RTL)
Example: Arabic text with English words
"Ù…Ø±Ø­Ø¨Ø§ John!"

Expected:
âœ“ Mixed text displays correctly
âœ“ English words left-to-right within RTL text
âœ“ Punctuation in correct position

TC-RTL-003: Numbers in RTL
Numbers: Always left-to-right
Example: "Ù¡Ù¢Ù£" = 123 (Arabic numerals)

Expected:
âœ“ Numbers display correctly
âœ“ Calculations work correctly

TC-CULTURE-010: Legal and regulatory
Different countries, different rules:

GDPR (Europe): Strict privacy laws
CCPA (California): Consumer privacy rights
Cookie laws: Must get consent (EU)
Age restrictions: Vary by country

Test:
âœ“ Privacy policy appropriate for region
âœ“ Cookie consent shown (if required)
âœ“ Age verification (if required)
âœ“ Terms of service localized

TC-CULTURE-011: Payment methods
Different regions prefer different methods:

US: Credit cards, PayPal
China: Alipay, WeChat Pay
India: UPI, Paytm
Germany: SEPA, Giropay
Netherlands: iDEAL

Test:
âœ“ Local payment methods supported
âœ“ Currency conversion correct
âœ“ Payment flow localized

TC-CULTURE-012: Holidays and weekends
Different calendars:

Western: Sunday/Monday start, Saturday/Sunday weekend
Middle East: Friday/Saturday weekend
India: Many regional holidays
China: Lunar New Year

Test:
âœ“ Calendar starts on correct day
âœ“ Weekends marked correctly
âœ“ Local holidays displayed

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### 1.4 Localization Testing Checklist

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
COMPREHENSIVE LOCALIZATION TESTING CHECKLIST
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

BEFORE TESTING:
â–¡ Identify target languages/locales
â–¡ Get native speakers for testing
â–¡ Understand cultural differences
â–¡ Review translation style guide
â–¡ Check competitor apps in target market

TRANSLATION TESTING:
â–¡ All UI text translated
â–¡ All error messages translated
â–¡ All help text / tooltips translated
â–¡ All email templates translated
â–¡ All push notifications translated
â–¡ No English leaking through
â–¡ No placeholder text (Lorem ipsum)
â–¡ Context-appropriate translations
â–¡ Industry terminology correct
â–¡ Tone/formality appropriate

LAYOUT TESTING:
â–¡ No text truncation
â–¡ No text overflow
â–¡ Buttons sized appropriately
â–¡ Dialogs resize for expanded text
â–¡ No overlapping elements
â–¡ Line breaks appropriate
â–¡ Proper spacing maintained
â–¡ Images and text don't overlap

FORMATTING TESTING:
â–¡ Date format correct
â–¡ Time format correct (12h/24h)
â–¡ Number format correct (decimal separator)
â–¡ Currency format and symbol correct
â–¡ Phone number format correct
â–¡ Address format correct for country
â–¡ Postal code format validated

CHARACTER TESTING:
â–¡ Special characters display
â–¡ Accented characters work
â–¡ Search with special characters works
â–¡ Input accepts special characters
â–¡ Sorting with special characters correct
â–¡ URLs/Email with special characters work

ENCODING TESTING:
â–¡ UTF-8 encoding throughout
â–¡ No garbled text (ï¿½ï¿½ï¿½ï¿½)
â–¡ All character sets display
â–¡ Database stores Unicode
â–¡ API handles Unicode
â–¡ Files save in UTF-8

FUNCTIONAL TESTING:
â–¡ Language switcher works
â–¡ Language persists after restart
â–¡ All features work in target language
â–¡ Forms submit correctly
â–¡ Validation messages in target language
â–¡ Login/Signup works

CULTURAL TESTING:
â–¡ Colors appropriate
â–¡ Images culturally appropriate
â–¡ Icons not offensive
â–¡ Symbols culturally correct
â–¡ Content tone appropriate
â–¡ Legal requirements met (GDPR, etc.)

RTL TESTING (Arabic, Hebrew):
â–¡ Text flows right-to-left
â–¡ UI mirrored correctly
â–¡ Navigation reversed
â–¡ Alignment correct (right-aligned)
â–¡ Scrollbars on left
â–¡ Icons mirrored appropriately
â–¡ Numbers still left-to-right

EDGE CASES:
â–¡ Very long words (German compounds)
â–¡ Short character sets (Chinese)
â–¡ Multiple scripts (Japanese: Hiragana, Katakana, Kanji)
â–¡ Mixed LTR-RTL content
â–¡ Special characters in usernames/passwords
â–¡ Unicode in URLs

PERFORMANCE:
â–¡ No performance degradation
â–¡ Font files load correctly
â–¡ Downloads complete
â–¡ No bloated file sizes

TESTING TOOLS:
â–¡ Pseudo-localization (testing tool)
â–¡ Screenshot comparison
â–¡ Translation management system (Crowdin, etc.)
â–¡ Native speakers for review

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## Accessibility Testing (WCAG Deep Dive)

### 2.1 WCAG Conformance Levels

**WCAG 2.1 Overview:**

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
WCAG 2.1 CONFORMANCE LEVELS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

LEVEL A (Minimum)
Basic web accessibility. If not met, some users CANNOT access content.

Key Requirements:
âœ“ Non-text content has text alternative
âœ“ Audio has captions
âœ“ Keyboard accessible
âœ“ No keyboard trap
âœ“ Page has title
âœ“ Link purpose clear
âœ“ Color not sole means of conveying information
âœ“ Audio control available

Example Issues if Level A not met:
- Blind user cannot access images (no alt text)
- Keyboard user trapped in modal (no escape)
- Deaf user cannot understand video (no captions)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

LEVEL AA (Mid-range, Most Common Target)
Addresses major barriers. Required by most laws (ADA, Section 508, etc.)

Additional Requirements:
âœ“ Color contrast 4.5:1 (normal text)
âœ“ Color contrast 3:1 (large text)
âœ“ Resize text to 200%
âœ“ Multiple ways to find content
âœ“ Headings and labels descriptive
âœ“ Focus visible
âœ“ Error identification
âœ“ Error suggestions provided
âœ“ Labels or instructions for user input

Example: Most companies target Level AA

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

LEVEL AAA (Highest, Gold Standard)
Enhanced accessibility. Difficult to achieve for all content.

Additional Requirements:
âœ“ Color contrast 7:1 (normal text)
âœ“ Color contrast 4.5:1 (large text)
âœ“ Audio described (extended)
âœ“ Sign language for audio
âœ“ No time limits OR very generous time limits
âœ“ No distractions (moving, blinking content)
âœ“ Context-sensitive help available

Example: Government websites often target AAA

Note: AAA is NOT required for full conformance.
Goal: Meet AAA where possible.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

CONFORMANCE STATEMENT:
To claim WCAG conformance, you must meet ALL criteria for that level.

Example:
"This website conforms to WCAG 2.1 Level AA"

Means: ALL Level A and ALL Level AA criteria met.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### 2.2 Detailed Accessibility Testing Scenarios

```


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PERCEIVABLE: Users must be able to perceive information
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1.1 TEXT ALTERNATIVES

TC-A11Y-001: Images of text
Bad: Using image for heading

Good HTML:
<h1>Welcome to Our Site</h1>

Bad HTML:
<img src="heading.png"> (no alt text, not scalable)

Expected:
âœ“ Real text used instead of images
âœ“ If image necessary, alt text provided
âœ“ Text can be resized by user

TC-A11Y-002: Informative images
Example: Product image, infographic

Expected:
<img src="product.jpg" alt="Red leather handbag with gold chain strap">

âœ“ Alt text describes image content
âœ“ Blind user understands what's shown
âœ“ Alt text concise but descriptive

TC-A11Y-003: Decorative images
Example: Border decorations, spacer images

Expected:
<img src="decoration.png" alt="" role="presentation">
OR
<img src="spacer.gif" alt="">

âœ“ Empty alt attribute (not missing)
âœ“ Screen reader skips image
âœ“ No "image" announcement

TC-A11Y-004: Complex images (charts, diagrams)
Example: Bar chart showing sales data

Expected:
<img src="sales-chart.png" alt="Bar chart showing quarterly sales">
<p>Detailed description: Q1 sales were $1.2M, Q2...</p>
OR
<a href="sales-data.html">View sales data table</a>

âœ“ Alt text provides overview
âœ“ Detailed description available
âœ“ Data also in accessible format (table)

TC-A11Y-005: Images of text (exceptions)
Acceptable cases:
- Logo
- Photo containing text
- Screenshot

Expected:
âœ“ Alt text includes text in image
Example: <img src="logo.png" alt="Company Name">

TC-A11Y-006: Audio content
Example: Podcast, audio announcement

Expected:
âœ“ Transcript provided
âœ“ Captions for important sounds
âœ“ Audio can be paused/stopped

TC-A11Y-007: Video content
Expected:
âœ“ Captions for dialogue
âœ“ Captions for important sounds ([Door closes], [Music playing])
âœ“ Audio description for visual-only content
âœ“ Transcript available

1.3 ADAPTABLE CONTENT

TC-A11Y-008: Semantic HTML
Bad:
<div class="heading">Page Title</div>
<div onclick="submit()">Submit</div>

Good:
<h1>Page Title</h1>
<button onclick="submit()">Submit</button>

Expected:
âœ“ Proper HTML elements (<h1>, <button>, <nav>, <main>)
âœ“ Screen reader announces element type
âœ“ Keyboard navigation works automatically

TC-A11Y-009: Heading hierarchy
Bad:
<h1>Main Title</h1>
<h3>Subsection</h3> (skipped h2!)

Good:
<h1>Main Title</h1>
<h2>Section</h2>
<h3>Subsection</h3>

Expected:
âœ“ No heading levels skipped
âœ“ One <h1> per page
âœ“ Logical document structure
âœ“ Screen reader users can navigate by headings

TC-A11Y-010: Reading order
Expected:
âœ“ Content order in HTML matches visual order
âœ“ CSS doesn't reorder illogically
âœ“ Keyboard navigation follows logical order

TC-A11Y-011: Sensory characteristics
Bad: "Click the green button on the right"

Good: "Click the Submit button"

Expected:
âœ“ Instructions don't rely solely on shape/color/position
âœ“ Multiple cues provided

TC-A11Y-012: Orientation
Expected:
âœ“ Content works in portrait and landscape
âœ“ No forced orientation (unless essential)
âœ“ Layout adapts to orientation

1.4 DISTINGUISHABLE

TC-A11Y-013: Color contrast (text)
Level AA Requirements:
- Normal text (< 18pt): 4.5:1 contrast
- Large text (â‰¥ 18pt or 14pt bold): 3:1 contrast

Level AAA Requirements:
- Normal text: 7:1 contrast
- Large text: 4.5:1 contrast

Tools: WebAIM Contrast Checker, Chrome DevTools

Test Examples:
âœ“ #333333 on #FFFFFF: 12.6:1 (Pass AA & AAA)
âœ“ #767676 on #FFFFFF: 4.5:1 (Pass AA, Fail AAA)
âœ— #999999 on #FFFFFF: 2.8:1 (Fail AA & AAA)

Expected:
âœ“ All text meets minimum contrast
âœ“ Links distinguishable from text
âœ“ Buttons have sufficient contrast

TC-A11Y-014: Color contrast (UI components)
Level AA: 3:1 contrast for UI components and graphical objects

Test:
âœ“ Form field borders: 3:1 contrast
âœ“ Focus indicators: 3:1 contrast
âœ“ Icons: 3:1 contrast
âœ“ Chart elements: 3:1 contrast

TC-A11Y-015: Use of color
Bad: "Required fields are in red"

Good: "Required fields are marked with an asterisk (*)"

Expected:
âœ“ Color not sole means of conveying information
âœ“ Additional indicators present (icons, text, patterns)
âœ“ Works for colorblind users

TC-A11Y-016: Text resize
Expected:
âœ“ Text can be resized to 200% without loss of content
âœ“ No horizontal scrolling (except data tables)
âœ“ Layout adapts to larger text
âœ“ No text truncation

TC-A11Y-017: Reflow
Expected:
âœ“ Content reflows at 400% zoom
âœ“ No two-dimensional scrolling required
âœ“ Mobile-friendly responsive design

TC-A11Y-018: Non-text contrast
Expected:
âœ“ Important graphics have 3:1 contrast
âœ“ UI component parts distinguishable
âœ“ Focus indicators visible

TC-A11Y-019: Text spacing
User applies:
- Line height: 1.5
- Paragraph spacing: 2x font size
- Letter spacing: 0.12x font size
- Word spacing: 0.16x font size

Expected:
âœ“ No loss of content
âœ“ No overlapping text
âœ“ Layout adapts

TC-A11Y-020: Content on hover or focus
Expected:
âœ“ Content dismissible (Esc key)
âœ“ Content hoverable (can move mouse to content)
âœ“ Content persists (doesn't disappear immediately)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
OPERABLE: Users must be able to operate the interface
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

2.1 KEYBOARD ACCESSIBLE

TC-A11Y-021: Keyboard access to all functionality
Expected:
âœ“ All features accessible via keyboard
âœ“ No mouse-only functionality
âœ“ Tab, Enter, Space, Arrow keys work

TC-A11Y-022: No keyboard trap
Test: Tab through entire page

Expected:
âœ“ Can always move focus
âœ“ Can tab out of every element
âœ“ Esc or standard key to exit modals

TC-A11Y-023: Character key shortcuts
If single-key shortcuts exist (e.g., "s" for search):

Expected:
âœ“ Can be turned off
âœ“ Can be remapped
âœ“ Only active when component focused

2.2 ENOUGH TIME

TC-A11Y-024: Session timeout
Expected:
âœ“ User warned before timeout (20-second warning)
âœ“ Option to extend session
âœ“ OR timeout at least 20 hours

TC-A11Y-025: Moving content (carousel, slideshow)
Expected:
âœ“ Can pause, stop, or hide
âœ“ Pauses on hover or focus
âœ“ No auto-play > 5 seconds (unless user can control)

2.3 SEIZURES AND PHYSICAL REACTIONS

TC-A11Y-026: Flashing content
Expected:
âœ— No content flashes more than 3 times per second
âœ“ OR flash below threshold (small, low contrast)

2.4 NAVIGABLE

TC-A11Y-027: Skip links
Expected:
<a href="#main" class="skip-link">Skip to main content</a>

âœ“ First focusable element
âœ“ Jumps to main content
âœ“ Bypasses navigation

TC-A11Y-028: Page title
Expected:
<title>Product Name - Page Title - Site Name</title>

âœ“ Unique per page
âœ“ Descriptive of page content
âœ“ Helps users identify page

TC-A11Y-029: Focus order
Expected:
âœ“ Focus order is logical
âœ“ Follows visual flow (top â†’ bottom, left â†’ right)
âœ“ No unexpected focus jumps

TC-A11Y-030: Link purpose
Bad: "Click here"

Good: "Download 2026 Annual Report (PDF, 2MB)"

Expected:
âœ“ Link text descriptive of destination
âœ“ Link purpose understood from text alone
âœ“ Or understood from context

TC-A11Y-031: Multiple ways to find content
Expected:
At least two of:
âœ“ Search function
âœ“ Site map
âœ“ Table of contents
âœ“ Navigation menu

TC-A11Y-032: Headings and labels
Expected:
âœ“ Headings describe topic/purpose
âœ“ Labels describe form fields
âœ“ Clear and descriptive

TC-A11Y-033: Focus visible
Expected:
âœ“ Keyboard focus always visible
âœ“ Clear visual indicator (outline, border, background)
âœ“ Minimum 3:1 contrast ratio
âœ“ Never hidden with outline:none without alternative

Good example:
*:focus {
    outline: 3px solid #0066CC;
    outline-offset: 2px;
}

TC-A11Y-034: Target size (touch)
Level AAA (but important for mobile):

Expected:
âœ“ Touch targets at least 44Ã—44 CSS pixels
âœ“ Adequate spacing between targets
âœ“ Easy to tap without mistakes

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
UNDERSTANDABLE: Users must be able to understand information
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

3.1 READABLE

TC-A11Y-035: Page language
Expected:
<html lang="en">

âœ“ Language of page specified
âœ“ Screen reader pronounces correctly
âœ“ Browser translation tools work

TC-A11Y-036: Language of parts
Expected:
<p>English text</p>
<p lang="es">Spanish text: Hola mundo</p>

âœ“ Language changes indicated
âœ“ Screen reader switches pronunciation

TC-A11Y-037: Unusual words
Expected:
âœ“ Jargon defined on first use
âœ“ Abbreviations spelled out first time
âœ“ Idioms explained
âœ“ Or link to glossary

3.2 PREDICTABLE

TC-A11Y-038: On focus
Expected:
âœ“ Focusing an element doesn't trigger unexpected change
âœ— No auto-submit on focus
âœ— No modal opening on focus
âœ“ Predictable behavior

TC-A11Y-039: On input
Expected:
âœ“ Entering data doesn't cause unexpected change
âœ— No auto-submit when field filled
âœ“ User must explicitly submit

Exception: Search autocomplete OK if user expects it

TC-A11Y-040: Consistent navigation
Expected:
âœ“ Navigation in same location/order on each page
âœ“ Same components work the same way throughout site
âœ“ Predictable patterns

TC-A11Y-041: Consistent identification
Expected:
âœ“ Same functionality labeled consistently
âœ“ "Search" doesn't become "Find" on different pages
âœ“ Icons have consistent meanings

3.3 INPUT ASSISTANCE

TC-A11Y-042: Error identification
Expected:
âœ“ Error detected automatically
âœ“ User notified clearly
âœ“ Error described in text
âœ“ Problem field identified

Example:
"Email is required"
"Email format is invalid"

TC-A11Y-043: Labels or instructions
Expected:
âœ“ Every form field has <label>
âœ“ Instructions provided when needed
âœ“ Required fields indicated

Good example:
<label for="email">Email Address *</label>
<input type="email" id="email" required>

TC-A11Y-044: Error suggestion
Expected:
âœ“ Error message suggests correction
âœ“ Example of correct format provided
âœ“ Help user fix error

Example:
"Password must be at least 8 characters, include 1 uppercase, 1 number"

TC-A11Y-045: Error prevention (legal, financial)
Expected:
At least one of:
âœ“ Reversible (can undo)
âœ“ Checked (data validated before submission)
âœ“ Confirmed (confirmation page, "Are you sure?")

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ROBUST: Content works with current and future tools
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

4.1 COMPATIBLE

TC-A11Y-046: Valid HTML
Expected:
âœ“ HTML validates (W3C validator)
âœ“ Complete start and end tags
âœ“ No duplicate IDs
âœ“ Proper nesting

TC-A11Y-047: Name, Role, Value
Expected:
âœ“ All UI components have accessible name
âœ“ Role communicated (button, link, etc.)
âœ“ State communicated (checked, expanded)
âœ“ Properties communicated (required, disabled)

ARIA example:
<button aria-label="Close dialog" aria-pressed="false">
    <span aria-hidden="true">Ã—</span>
</button>

TC-A11Y-048: Status messages
Expected:
âœ“ Status messages announced to screen reader
âœ“ User doesn't need to be on that element

Using ARIA live regions:
<div aria-live="polite" role="status">
    Items added to cart
</div>

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### 2.3 ARIA (Accessible Rich Internet Applications)

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ARIA ATTRIBUTES COMPREHENSIVE GUIDE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ARIA RULES:
1. Use native HTML when possible (better than ARIA)
2. Don't change native semantics
3. All interactive ARIA controls must be keyboard accessible
4. Don't use role="presentation" or aria-hidden on focusable elements
5. All interactive elements must have accessible name

ARIA ROLES:

TC-ARIA-001: Landmark roles
Purpose: Structure page for screen reader navigation

<header role="banner">Site header</header>
<nav role="navigation">Main navigation</nav>
<main role="main">Main content</main>
<aside role="complementary">Sidebar</aside>
<footer role="contentinfo">Site footer</footer>

Expected:
âœ“ Screen reader can jump to landmarks
âœ“ One banner, one main, one contentinfo per page
âœ“ Multiple navigations OK (label differently)

TC-ARIA-002: Widget roles
Purpose: Define interactive components

<div role="button" tabindex="0">Custom Button</div>
<div role="dialog" aria-modal="true">Modal Dialog</div>
<div role="alert">Important Alert</div>
<div role="tablist">
    <button role="tab">Tab 1</button>
    <button role="tab">Tab 2</button>
</div>

Expected:
âœ“ Role announced by screen reader
âœ“ Keyboard interaction matches role
âœ“ Native HTML preferred when available

ARIA PROPERTIES (aria-*):

TC-ARIA-003: aria-label
Purpose: Provide accessible name

<button aria-label="Close dialog">
    <span aria-hidden="true">Ã—</span>
</button>

Expected:
âœ“ Screen reader announces label
âœ“ Useful for icon-only buttons
âœ“ Overrides visible text

TC-ARIA-004: aria-labelledby
Purpose: Reference another element as label

<h2 id="dialog-title">Confirm Delete</h2>
<div role="dialog" aria-labelledby="dialog-title">
    Are you sure?
</div>

Expected:
âœ“ Screen reader announces referenced element's text
âœ“ Multiple IDs supported (space-separated)

TC-ARIA-005: aria-describedby
Purpose: Additional description/help text

<input type="email" id="email" aria-describedby="email-help">
<small id="email-help">We'll never share your email</small>

Expected:
âœ“ Screen reader announces description after label
âœ“ Helpful for instructions

TC-ARIA-006: aria-hidden
Purpose: Hide content from screen readers

<span aria-hidden="true">â˜…</span> 4.5 stars

Expected:
âœ“ Decorative icons hidden
âœ“ Visual-only content hidden
âœ— Don't hide focusable elements
âœ— Don't hide important information

TC-ARIA-007: aria-live
Purpose: Announce dynamic content updates

<div aria-live="polite" aria-atomic="true">
    3 items in cart
</div>

Values:
- polite: Announce when user is idle
- assertive: Announce immediately
- off: Don't announce (default)

Expected:
âœ“ Updates announced automatically
âœ“ User doesn't need to navigate to element
âœ“ Use for status messages

TC-ARIA-008: aria-expanded
Purpose: Indicate if element expanded/collapsed

<button aria-expanded="false" aria-controls="menu">
    Menu
</button>
<ul id="menu" hidden>...</ul>

Expected:
âœ“ Screen reader announces state
âœ“ Value toggles when state changes
âœ“ Use for dropdowns, accordions

TC-ARIA-009: aria-current
Purpose: Indicate current item in set

<nav>
    <a href="/">Home</a>
    <a href="/about" aria-current="page">About</a>
    <a href="/contact">Contact</a>
</nav>

Expected:
âœ“ Screen reader announces "current page"
âœ“ Use for active navigation item

TC-ARIA-010: aria-required
Purpose: Indicate required form field

<label for="email">Email *</label>
<input type="email" id="email" aria-required="true">

Expected:
âœ“ Screen reader announces "required"
âœ“ Use with required attribute

TC-ARIA-011: aria-invalid
Purpose: Indicate validation error

<input type="email" id="email" aria-invalid="true"
       aria-describedby="email-error">
<span id="email-error">Email format is invalid</span>

Expected:
âœ“ Screen reader announces "invalid"
âœ“ Error message associated and announced

TC-ARIA-012: aria-disabled vs disabled
Difference:
disabled: Truly disabled, not focusable, grayed out
aria-disabled="true": Appears disabled, but can be focused

Expected:
âœ“ Use disabled for most cases
âœ“ Use aria-disabled when need to explain why disabled

TC-ARIA-013: aria-modal
Purpose: Indicate modal dialog

<div role="dialog" aria-modal="true" aria-labelledby="title">
    <h2 id="title">Confirm Action</h2>
    ...
</div>

Expected:
âœ“ Focus trapped in modal
âœ“ Background content not accessible
âœ“ Esc key closes modal

COMMON ARIA PATTERNS:

TC-ARIA-014: Modal dialog complete example
<div role="dialog"
     aria-modal="true"
     aria-labelledby="modal-title"
     aria-describedby="modal-desc">

    <h2 id="modal-title">Delete Item</h2>
    <p id="modal-desc">This action cannot be undone.</p>

    <button>Cancel</button>
    <button>Delete</button>
</div>

Expected:
âœ“ Focus moves to modal on open
âœ“ Focus trapped inside
âœ“ Esc closes modal
âœ“ Focus returns to trigger on close

TC-ARIA-015: Accordion complete example
<div class="accordion">
    <h3>
        <button aria-expanded="false"
                aria-controls="panel1"
                id="accordion1">
            Section 1
        </button>
    </h3>
    <div id="panel1"
         role="region"
         aria-labelledby="accordion1"
         hidden>
        Content
    </div>
</div>

Expected:
âœ“ Button announces expanded state
âœ“ Arrow keys navigate between buttons
âœ“ Panel hidden when collapsed

TC-ARIA-016: Tab panel complete example
<div role="tablist" aria-label="Sample Tabs">
    <button role="tab"
            aria-selected="true"
            aria-controls="panel-1"
            id="tab-1">
        Tab 1
    </button>
    <button role="tab"
            aria-selected="false"
            aria-controls="panel-2"
            id="tab-2"
            tabindex="-1">
        Tab 2
    </button>
</div>

<div role="tabpanel"
     id="panel-1"
     aria-labelledby="tab-1">
    Panel 1 content
</div>

<div role="tabpanel"
     id="panel-2"
     aria-labelledby="tab-2"
     hidden>
    Panel 2 content
</div>

Expected:
âœ“ Tab key moves to tablist
âœ“ Arrow keys navigate between tabs
âœ“ Active tab has aria-selected="true"
âœ“ Inactive tabs have tabindex="-1"

TC-ARIA-017: Combobox (autocomplete) example
<label for="search">Search</label>
<input type="text"
       id="search"
       role="combobox"
       aria-autocomplete="list"
       aria-expanded="false"
       aria-controls="suggestions">

<ul role="listbox" id="suggestions" hidden>
    <li role="option">Suggestion 1</li>
    <li role="option">Suggestion 2</li>
</ul>

Expected:
âœ“ Typing shows suggestions
âœ“ aria-expanded toggles
âœ“ Arrow keys navigate suggestions
âœ“ Enter selects suggestion

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## Usability Testing

### 3.1 Usability Testing Fundamentals

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
USABILITY TESTING OVERVIEW
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

DEFINITION:
Usability testing = Observing users as they interact with a product to identify usability issues and gather qualitative data about their experience.

GOAL:
âœ“ Ensure product is easy to use
âœ“ Identify confusing areas
âœ“ Improve user experience
âœ“ Validate design decisions

KEY DIFFERENCE FROM FUNCTIONAL TESTING:
Functional: Does it work?
Usability: Is it easy to use?

Example:
Functional Test: User can login (Pass âœ“)
Usability Test: User finds login button intuitive and completes login quickly (Fail âœ— - took 2 minutes to find button)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

USABILITY TESTING METHODS:

1. MODERATED USABILITY TESTING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Definition: Facilitator guides user through tasks in real-time

Process:
1. Recruit participants (5-8 users)
2. Prepare test scenarios
3. Conduct sessions (1 hour each)
4. Observe and take notes
5. Ask follow-up questions
6. Analyze findings

Advantages:
âœ“ Can probe deeper (ask why)
âœ“ Understand user thought process
âœ“ Clarify confusion
âœ“ Observe body language

Disadvantages:
âœ— Time-consuming
âœ— Expensive
âœ— Facilitator bias possible

2. UNMODERATED USABILITY TESTING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Definition: Users complete tasks independently, remote

Tools: UserTesting.com, Lookback, UsabilityHub

Process:
1. Create tasks and questions
2. Recruit participants
3. Users complete tasks at their own time
4. Review recordings

Advantages:
âœ“ Faster
âœ“ Cheaper
âœ“ More participants
âœ“ Natural environment

Disadvantages:
âœ— Cannot probe deeper
âœ— Technical issues possible
âœ— Less contextual information

3. GUERRILLA TESTING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Definition: Quick, informal testing in public places (coffee shop, library)

Process:
1. Bring laptop/device
2. Approach people
3. Ask 5-10 minutes of their time
4. Have them complete 1-2 tasks
5. Gather quick feedback

Advantages:
âœ“ Very fast
âœ“ Very cheap
âœ“ Real people
âœ“ Quick validation

Disadvantages:
âœ— Not your target audience necessarily
âœ— Superficial feedback
âœ— Short sessions

4. REMOTE USABILITY TESTING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Definition: Testing with users in different locations

Tools: Zoom, Teams, UserTesting

Advantages:
âœ“ Geographic diversity
âœ“ Users in natural environment
âœ“ No travel required
âœ“ Record sessions easily

Disadvantages:
âœ— Technical challenges
âœ— Harder to observe non-verbal cues
âœ— Connectivity issues

5. A/B TESTING (Quantitative)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Definition: Comparing two versions to see which performs better

Example:
Version A: "Buy Now" button (green)
Version B: "Add to Cart" button (blue)

Metrics:
- Click-through rate
- Conversion rate
- Time on page

Advantages:
âœ“ Data-driven
âœ“ Clear winner
âœ“ Live user data

Disadvantages:
âœ— Doesn't explain why
âœ— Requires significant traffic
âœ— Only tests one variable at a time

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### 3.2 Conducting Usability Tests

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
USABILITY TEST PROCESS (MODERATED)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PHASE 1: PREPARATION (Before Test)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 1: Define Objectives
What do you want to learn?
Examples:
- Can users complete checkout in < 2 minutes?
- Do users understand the navigation?
- Is the search function intuitive?

Step 2: Recruit Participants
How many: 5-8 users (Nielsen: 5 users find 85% of issues)
Who: Representative of target audience
Where: usertesting.com, respondent.io, or your user base

Screening questions example:
- Age range?
- Tech proficiency (1-5)?
- Have you used [competitor product]?
- How often do you shop online?

Step 3: Prepare Test Scenarios
Create realistic tasks, not instructions

- âŒ Bad: "Click the search icon in the top right and search for 'laptop'"
- âœ… Good: "You're looking for a laptop for college. Find one that costs under $800."

5-7 tasks per session

Example Tasks (E-commerce site):
1. Find a blue t-shirt in size medium
2. Add it to your cart
3. Apply a discount code
4. Complete the checkout process
5. Track your order

Step 4: Prepare Test Environment
â–¡ Quiet room
â–¡ Recording equipment (screen + audio)
â–¡ Test device (laptop, phone, etc.)
â–¡ Consent form
â–¡ Note-taking materials

Step 5: Create Test Script
Welcome Script:
"Thank you for participating. Today we're testing the website, not you. There are no right or wrong answers. Please think aloud as you complete tasks. Do you have any questions?"

Consent:
"We'll record your screen and voice. Is that OK?"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PHASE 2: DURING TEST (Conducting Session)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Introduction (5 minutes)
âœ“ Welcome participant
âœ“ Explain purpose
âœ“ Get consent
âœ“ Encourage thinking aloud
âœ“ Ask demographic questions

2. Tasks (40 minutes)
For each task:
a) Read task aloud
b) Let user attempt
c) Observe (don't help!)
d) Take notes
e) Ask follow-up questions

THINKING ALOUD PROTOCOL:
Ask user to verbalize thoughts as they work
"What are you looking for?"
"What do you expect to happen when you click that?"

If user goes silent:
"What are you thinking?"
"Tell me what you see."

PROBING QUESTIONS (After task):
"How was that?"
"What was confusing?"
"What did you expect?"
"On a scale 1-5, how easy was that?"

IMPORTANT:
âœ— Don't lead: "Was the button hard to find?" (leading)
âœ“ Open-ended: "How did you find the button?" (neutral)

3. Wrap-up (10 minutes)
âœ“ Overall impressions
âœ“ What did you like?
âœ“ What was frustrating?
âœ“ Any suggestions?
âœ“ Thank participant

4. Observations to Note:
â–¡ Task completion (success/failure)
â–¡ Time taken per task
â–¡ Number of errors
â–¡ Hesitations
â–¡ Confusion (where/why)
â–¡ Unexpected paths taken
â–¡ Comments (positive/negative)
â–¡ Body language (frustration, delight)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PHASE 3: ANALYSIS (After Tests)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 1: Review Recordings
Watch all sessions
Take detailed notes

Step 2: Identify Patterns
Look for issues that occurred in multiple sessions
- 4/5 users couldn't find search â†’ Critical issue
- 2/5 users confused by checkout â†’ Medium issue
- 1/5 user suggested feature â†’ Note, not critical

Step 3: Severity Rating
Critical (P0): Blocks task completion
- 4 users couldn't complete checkout
- All users got lost in navigation

High (P1): Major frustration, but workaround exists
- 3 users took > 2 minutes to find setting
- Users complained about confusing labels

Medium (P2): Minor inconvenience
- 2 users didn't notice a feature
- Button could be more prominent

Low (P3): Suggestions, nice-to-haves
- 1 user suggested adding a feature
- Color preference

Step 4: Create Findings Report
Include:
1. Executive Summary
   - Key findings
   - Critical issues (3-5 top issues)
   - Recommendations

2. Methodology
   - Number of participants
   - Date of tests
   - Tasks tested

3. Detailed Findings
   For each issue:
   - Description
   - Severity
   - Number of users affected
   - Evidence (quotes, screenshots)
   - Recommendation

4. Quantitative Data
   - Task success rates
   - Average time per task
   - Error rates

5. Qualitative Data
   - User quotes
   - Positive feedback
   - Suggestions

Example Finding:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ISSUE #1: Users cannot find the search function
Severity: Critical (P0)
Affected: 5/5 participants

Description:
All participants struggled to find the search function. The search icon is in the top right corner but is not prominent. 3 participants eventually found it after 1-2 minutes. 2 participants gave up and used the navigation menu instead.

Evidence:
- P1: "I don't see a search bar anywhere. Where is it?"
- P3: "Oh, that tiny icon is the search? I didn't notice it."
- P5: "I expect search to be more obvious. I almost missed it."

Impact:
- Average time to find search: 87 seconds
- Task success rate: 40% (2/5 completed using search)
- All users expressed frustration

Recommendation:
1. Make search icon larger and more prominent
2. Add "Search" text label next to icon
3. Consider adding search bar to header (always visible)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 5: Prioritize Fixes
Work with team to prioritize based on:
- Severity
- Frequency (how many users affected)
- Effort to fix
- Business impact

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### 3.3 Usability Metrics

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
USABILITY METRICS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. TASK SUCCESS RATE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Definition: Percentage of tasks completed successfully

Formula: (Tasks Completed / Total Task Attempts) Ã— 100%

Example:
5 users, 5 tasks each = 25 total task attempts
22 tasks completed successfully
Success Rate = 22/25 Ã— 100% = 88%

Benchmark:
âœ“ > 90%: Excellent
âœ“ 75-90%: Good
âš  50-75%: Needs improvement
âœ— < 50%: Critical issues

2. TIME ON TASK
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Definition: How long it takes to complete a task

Measure:
- Average time
- Median time (better for outliers)
- Time range (fastest to slowest)

Example:
Task: Complete checkout

User 1: 2:15 (2 minutes, 15 seconds)
User 2: 1:45
User 3: 3:30
User 4: 2:00
User 5: 4:00

Average: 2:42
Median: 2:15

Interpretation:
Compare to:
- Baseline (previous version)
- Competitor
- Target (e.g., < 2 minutes)

3. ERROR RATE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Definition: Number of errors made during tasks

Types of Errors:
- Critical: Prevents task completion
- Non-critical: Slows down but recoverable

Formula: (Total Errors / Total Tasks) Ã— 100

Example:
25 tasks attempted
8 errors (3 critical, 5 non-critical)
Error Rate = 8/25 Ã— 100 = 32%

Target: < 10% error rate

4. SUBJECTIVE SATISFACTION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Definition: User's perception of ease of use

Measurement Methods:

A) Single Ease Question (SEQ)
After each task:
"Overall, how difficult or easy was this task to complete?"
1 (Very Difficult) to 7 (Very Easy)

Average score across users

B) System Usability Scale (SUS)
10 questions, 5-point scale (Strongly Disagree to Strongly Agree)

Questions:
1. I think I would like to use this system frequently
2. I found the system unnecessarily complex
3. I thought the system was easy to use
4. I think I would need support to use this system
5. I found the various functions were well integrated
6. I thought there was too much inconsistency
7. I would imagine most people would learn this quickly
8. I found the system very cumbersome to use
9. I felt very confident using the system
10. I needed to learn a lot before I could get going

Scoring: 0-100
- > 80: Excellent
- 68-80: Good (68 is average)
- < 68: Needs improvement

C) Net Promoter Score (NPS)
"How likely are you to recommend this product?"
0 (Not at all likely) to 10 (Extremely likely)

Calculation:
% Promoters (9-10) - % Detractors (0-6) = NPS

Example:
40% Promoters, 10% Detractors
NPS = 40 - 10 = 30

Score Interpretation:
- > 50: Excellent
- 0-49: Good
- < 0: Needs improvement

5. TASK LEVEL SATISFACTION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
After each task:
"How satisfied are you with your experience completing this task?"
1 (Very Dissatisfied) to 5 (Very Satisfied)

Average across tasks and users

6. FIRST CLICK
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Definition: Where did user click first when attempting task?

Metric: % of users who clicked correct first element

Example:
Task: Find the search function
5 users:
- 2 clicked search icon (correct) â†’ 40%
- 2 clicked menu â†’ 40%
- 1 clicked logo â†’ 20%

First Click Success Rate = 40%

Insight: If first click wrong, task completion decreases significantly

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### 3.4 Nielsen's 10 Usability Heuristics

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
NIELSEN'S 10 USABILITY HEURISTICS
(Jakob Nielsen, 1994 - Still relevant in 2026)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. VISIBILITY OF SYSTEM STATUS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Principle: System should always inform users about what's going on

Examples:
âœ“ Loading spinner while page loads
âœ“ Progress bar during file upload
âœ“ "Saving..." indicator
âœ“ "3 items in cart" notification
âœ“ Breadcrumbs showing current location

âœ— Bad: Silent loading (user doesn't know if anything's happening)
âœ“ Good: "Loading products..." with spinner

Test:
- Perform action, observe feedback
- All actions should have visible feedback within 1 second

2. MATCH BETWEEN SYSTEM AND REAL WORLD
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Principle: Use familiar language and concepts

Examples:
âœ“ "Shopping cart" (familiar metaphor)
âœ“ "Trash" or "Recycle bin" for delete
âœ“ Calendar icon for dates
âœ“ Magnifying glass for search

âœ— Bad: Technical jargon ("Initialize DB schema")
âœ“ Good: Plain language ("Set up your account")

Test:
- Review all labels, buttons, messages
- Are terms familiar to target users?
- No technical jargon for non-technical users

3. USER CONTROL AND FREEDOM
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Principle: Users often perform actions by mistake. Provide "emergency exit"

Examples:
âœ“ Undo button
âœ“ Cancel button on forms
âœ“ Back button always available
âœ“ "Are you sure?" confirmation for destructive actions
âœ“ Ability to recover deleted items

âœ— Bad: No way to cancel subscription
âœ“ Good: "Cancel Subscription" button with confirmation

Test:
- Try to undo actions
- Look for cancel/back options
- Test "emergency exits"

4. CONSISTENCY AND STANDARDS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Principle: Follow platform conventions and be internally consistent

Examples:
âœ“ Submit buttons always same color/style
âœ“ Navigation always in same place
âœ“ Icons have consistent meanings
âœ“ Follow iOS/Android guidelines on mobile

âœ— Bad: "Submit" on one page, "Send" on another (same function)
âœ“ Good: "Submit" consistently throughout

Test:
- Navigate through all pages
- Check consistency of:
  - Button styles
  - Navigation placement
  - Terminology
  - Interaction patterns

5. ERROR PREVENTION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Principle: Prevent errors before they occur

Examples:
âœ“ Disable "Submit" until form valid
âœ“ Date picker instead of text input
âœ“ Dropdown instead of free text
âœ“ Confirmation: "Are you sure you want to delete?"
âœ“ Input masks (phone: (555) 555-5555)

âœ— Bad: Let user type invalid date, then show error
âœ“ Good: Date picker ensures valid date

Test:
- Try to enter invalid data
- Look for safeguards preventing errors
- Check for confirmations on destructive actions

6. RECOGNITION RATHER THAN RECALL
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Principle: Minimize user's memory load. Make objects, actions, options visible.

Examples:
âœ“ Show recently viewed items
âœ“ Autocomplete in search
âœ“ Show order history (don't make user remember order number)
âœ“ Visual menus instead of commands

âœ— Bad: "Enter command" (requires memorization)
âœ“ Good: Visual menu with icons and labels

Test:
- Can user see options rather than remember them?
- Are previous choices visible?
- Is navigation visible?

7. FLEXIBILITY AND EFFICIENCY OF USE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Principle: Cater to both novice and expert users

Examples:
âœ“ Keyboard shortcuts for power users
âœ“ "Quick checkout" for returning customers
âœ“ Customizable dashboard
âœ“ Recently used items list

âœ— Bad: One workflow fits all
âœ“ Good: Express checkout for registered users, full flow for guests

Test:
- Complete tasks as novice (first time)
- Complete tasks as expert (repeat)
- Should be faster second time
- Look for shortcuts, saved preferences

8. AESTHETIC AND MINIMALIST DESIGN
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Principle: Don't include irrelevant information. Every extra unit of information competes with relevant units.

Examples:
âœ“ Clean, uncluttered interface
âœ“ Only essential information visible
âœ“ Hide advanced options behind "Advanced"
âœ“ Progressive disclosure (show details on demand)

âœ— Bad: Homepage with 50 links, 10 banners, multiple pop-ups
âœ“ Good: Clean homepage, clear value proposition, few key actions

Test:
- Is there unnecessary information?
- Can anything be removed without losing functionality?
- Is visual hierarchy clear?

9. HELP USERS RECOGNIZE, DIAGNOSE, AND RECOVER FROM ERRORS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Principle: Error messages in plain language, indicate problem, suggest solution

Examples:
âœ— Bad: "Error 404"
âœ“ Good: "Page not found. The page you're looking for doesn't exist. Try our homepage."

âœ— Bad: "Invalid input"
âœ“ Good: "Email format is invalid. Please enter a valid email like name@example.com"

âœ— Bad: "System error 0x8004FE21"
âœ“ Good: "We couldn't process your payment. Please check your card details and try again."

Test:
- Trigger errors intentionally
- Check error messages:
  - Plain language?
  - Explain what happened?
  - Suggest how to fix?

10. HELP AND DOCUMENTATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Principle: Best if system doesn't need explanation, but provide help if needed

Examples:
âœ“ Context-sensitive help (? icon next to complex fields)
âœ“ Tooltips on hover
âœ“ FAQ section
âœ“ Search in help section
âœ“ Video tutorials

âœ— Bad: No help available, users must figure it out
âœ“ Good: Help icon available, comprehensive FAQ, searchable

Test:
- Try to find help when confused
- Is help easy to find?
- Is help actually helpful?
- Can search help content?

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---



## Compatibility Testing

### 4.1 Browser Compatibility Testing (Deep Dive)

```


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
BROWSER COMPATIBILITY TESTING (COMPREHENSIVE)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

BROWSER MARKET SHARE (2026):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Desktop:
1. Chrome: 63%
2. Safari: 11%
3. Edge: 11%
4. Firefox: 5%
5. Opera: 3%
6. Others: 7%

Mobile:
1. Chrome: 63%
2. Safari: 32%
3. Samsung Internet: 4%
4. Others: 1%

TESTING STRATEGY:
P0: Chrome (latest 2 versions), Safari (latest)
P1: Edge (latest), Firefox (latest)
P2: Older versions (1-2 versions back)
P3: Opera, Samsung Internet, UC Browser

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

BROWSER-SPECIFIC ISSUES:

TC-COMPAT-001: CSS Grid Layout
Support: Chrome 57+, Firefox 52+, Safari 10.1+, Edge 16+

Test:
.container {
    display: grid;
    grid-template-columns: repeat(3, 1fr);
}

Expected:
âœ“ Layout works in modern browsers
âœ“ Fallback for older browsers (flexbox or float)

TC-COMPAT-002: CSS Custom Properties (Variables)
Support: All modern browsers

Test:
:root {
    --primary-color: #0066CC;
}
.button {
    background: var(--primary-color);
}

Expected:
âœ“ Works in modern browsers
âœ“ Fallback color for older browsers

TC-COMPAT-003: JavaScript ES6+ Features
Features: Arrow functions, let/const, template literals, etc.

Test:
const add = (a, b) => a + b;
const message = `Hello, ${name}!`;

Expected:
âœ“ Works in modern browsers
âœ“ Transpiled to ES5 for older browsers (using Babel)

TC-COMPAT-004: Fetch API
Support: All modern browsers, not IE11

Test:
fetch('/api/data')
    .then(response => response.json())
    .then(data => console.log(data));

Expected:
âœ“ Works in modern browsers
âœ“ Polyfill or fallback (XMLHttpRequest) for IE11

TC-COMPAT-005: WebP Image Format
Support: Chrome, Edge, Firefox, Safari 14+

Test:
<picture>
    <source srcset="image.webp" type="image/webp">
    <img src="image.jpg" alt="Description">
</picture>

Expected:
âœ“ WebP loads in supporting browsers
âœ“ JPG fallback for others

TC-COMPAT-006: CSS Flexbox
Support: All modern browsers, IE10+ (with prefixes)

Test:
.container {
    display: -webkit-flex; /* Safari */
    display: -ms-flexbox; /* IE10 */
    display: flex;
}

Expected:
âœ“ Works in all browsers with prefixes

TC-COMPAT-007: Video/Audio Formats
Different browsers support different codecs

Video:
- MP4 (H.264): All browsers
- WebM: Chrome, Firefox, Edge
- Ogg: Firefox, Chrome

Test:
<video>
    <source src="video.mp4" type="video/mp4">
    <source src="video.webm" type="video/webm">
    Your browser doesn't support video
</video>

Expected:
âœ“ Video plays in all browsers (using appropriate format)

TC-COMPAT-008: Local Storage
Support: All modern browsers

Test:
localStorage.setItem('key', 'value');
const value = localStorage.getItem('key');

Expected:
âœ“ Works in all browsers
âœ“ Graceful handling if disabled (private browsing)

TC-COMPAT-009: Service Workers (PWA)
Support: Chrome, Firefox, Edge, Safari 11.1+

Test: Register service worker

Expected:
âœ“ Works in supporting browsers
âœ“ Graceful degradation in others (app works without PWA features)

TC-COMPAT-010: CSS Position: Sticky
Support: All modern browsers

Test:
.header {
    position: sticky;
    top: 0;
}

Expected:
âœ“ Sticky header in modern browsers
âœ“ Fallback (position: fixed) in older browsers

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

CROSS-BROWSER TESTING CHECKLIST:

LAYOUT & VISUAL:
â–¡ Page layout consistent across browsers
â–¡ Fonts render correctly (web fonts load)
â–¡ Colors accurate
â–¡ Spacing and alignment correct
â–¡ Images display correctly
â–¡ Icons/SVGs render
â–¡ Animations smooth
â–¡ Responsive breakpoints work

FUNCTIONALITY:
â–¡ All forms submit correctly
â–¡ All links work
â–¡ Buttons clickable and functional
â–¡ Dropdown menus work
â–¡ Modals/popups display correctly
â–¡ JavaScript functions correctly
â–¡ AJAX requests succeed
â–¡ Cookies work
â–¡ Local storage works
â–¡ Session management works

PERFORMANCE:
â–¡ Page load time acceptable on each browser
â–¡ No memory leaks
â–¡ Smooth scrolling
â–¡ No browser crashes or freezes

CONSOLE ERRORS:
â–¡ No JavaScript errors (F12 â†’ Console)
â–¡ No CSS warnings
â–¡ No 404 errors (missing resources)
â–¡ No CORS errors

SPECIFIC FEATURES:
â–¡ HTML5 features work (if used)
â–¡ CSS3 features render (if used)
â–¡ ES6+ JavaScript works (or transpiled)
â–¡ Browser-specific APIs work or have fallbacks

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

TESTING TOOLS:

1. BROWSERSTACK (Cloud Testing)
- Test on 3000+ real browsers/devices
- Price: From $29/month
- Best for: Comprehensive testing

2. LAMBDATEST (Cloud Testing)
- 3000+ browser/OS combinations
- Price: From $15/month
- Best for: Budget-friendly

3. CROSSBROWSERTESTING (Cloud Testing)
- Live testing + automated
- Price: From $29/month
- Best for: Visual testing

4. SAUCE LABS (Cloud Testing)
- Enterprise-grade testing
- Price: Custom
- Best for: Large teams, CI/CD

5. CAN I USE (caniuse.com)
- Browser support tables
- Free
- Best for: Checking feature support

6. BROWSERLING (Quick Testing)
- Quick browser testing
- Free tier available
- Best for: Quick checks

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

BROWSER TESTING MATRIX EXAMPLE:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature     â”‚Chrome  â”‚Safari  â”‚Firefox â”‚ Edge   â”‚ IE11   â”‚
â”‚             â”‚120     â”‚17      â”‚120     â”‚120     â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Homepage    â”‚ âœ“ Pass â”‚ âœ“ Pass â”‚ âœ“ Pass â”‚ âœ“ Pass â”‚ N/A    â”‚
â”‚ Login       â”‚ âœ“ Pass â”‚ âœ“ Pass â”‚ âœ“ Pass â”‚ âœ“ Pass â”‚ N/A    â”‚
â”‚ Search      â”‚ âœ“ Pass â”‚ âœ“ Pass â”‚ âœ“ Pass â”‚ âœ“ Pass â”‚ N/A    â”‚
â”‚ Checkout    â”‚ âœ“ Pass â”‚ âš  Minorâ”‚ âœ— Fail â”‚ âœ“ Pass â”‚ N/A    â”‚
â”‚ Profile     â”‚ âœ“ Pass â”‚ âœ“ Pass â”‚ âœ“ Pass â”‚ âœ“ Pass â”‚ N/A    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Issues:
- Safari: Animation stutter on checkout page
- Firefox: Payment button styling issue

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### 4.2 Operating System Compatibility

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
OPERATING SYSTEM COMPATIBILITY TESTING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

DESKTOP OPERATING SYSTEMS:

TC-OS-001: Windows Testing
Versions to test:
- Windows 11 (latest)
- Windows 10 (still 50%+ users)

Browsers:
- Chrome, Edge (Chromium), Firefox

Test:
âœ“ Application launches
âœ“ UI renders correctly
âœ“ File operations work (open, save, download)
âœ“ Keyboard shortcuts work
âœ“ Right-click context menu works
âœ“ Installation/uninstallation works
âœ“ Windows notifications work

TC-OS-002: macOS Testing
Versions to test:
- macOS Sonoma (14.x) - latest
- macOS Ventura (13.x) - previous

Browsers:
- Safari, Chrome, Firefox

Test:
âœ“ Application launches
âœ“ UI renders correctly (Retina display)
âœ“ Keyboard shortcuts work (âŒ˜ instead of Ctrl)
âœ“ Trackpad gestures work
âœ“ macOS notifications work
âœ“ Integration with macOS features (Spotlight, etc.)

TC-OS-003: Linux Testing
Distributions to test:
- Ubuntu (most popular)
- Fedora
- Debian

Browsers:
- Chrome/Chromium, Firefox

Test:
âœ“ Application launches
âœ“ Dependencies installed correctly
âœ“ UI renders correctly
âœ“ Keyboard shortcuts work
âœ“ File operations work
âœ“ Package managers work (apt, yum, etc.)

MOBILE OPERATING SYSTEMS:

TC-OS-004: iOS Testing
Versions to test:
- iOS 18 (latest)
- iOS 17 (still common)
- iOS 16 (minimum supported)

Test:
âœ“ App launches on all versions
âœ“ UI adapts to different screen sizes
âœ“ Gestures work correctly
âœ“ iOS-specific features work (Face ID, etc.)
âœ“ Notifications work
âœ“ Permissions requested correctly

TC-OS-005: Android Testing
Versions to test:
- Android 14 (latest)
- Android 13 (common)
- Android 12 (still 15% users)
- Android 11 (minimum supported)

Test:
âœ“ App launches on all versions
âœ“ UI adapts to different screen sizes
âœ“ Back button behavior correct
âœ“ Android-specific features work
âœ“ Notifications work
âœ“ Permissions requested correctly

CROSS-OS ISSUES:

TC-OS-006: Font rendering differences
Windows: ClearType
macOS: Quartz
Linux: FreeType

Test:
âœ“ Fonts readable on all OSes
âœ“ No font-size/layout issues
âœ“ Web fonts load correctly

TC-OS-007: File path differences
Windows: C:\Users\Name\file.txt (backslashes)
macOS/Linux: /Users/Name/file.txt (forward slashes)

Test:
âœ“ File operations handle paths correctly
âœ“ No hardcoded path separators

TC-OS-008: Keyboard shortcuts
Windows/Linux: Ctrl+C, Ctrl+V
macOS: âŒ˜+C, âŒ˜+V

Test:
âœ“ Shortcuts work on each platform
âœ“ Platform-specific shortcuts respected

TC-OS-009: Screen DPI/Scaling
Windows: 100%, 125%, 150%, 175%, 200%
macOS: Retina (2x, 3x)

Test:
âœ“ UI scales correctly
âœ“ Images sharp at all DPIs
âœ“ No layout breaks

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## Summary & Key Takeaways

### Day 11 Key Concepts:

1. **Localization & Internationalization:**
   - i18n (architecture) vs L10n (adaptation)
   - Translation completeness and accuracy
   - Text expansion/contraction issues
   - Date/time/currency/number formatting
   - RTL language support (Arabic, Hebrew)
   - Cultural adaptation (colors, symbols, addresses)

2. **Accessibility Testing:**
   - WCAG 2.1 conformance levels (A, AA, AAA)
   - Four principles: Perceivable, Operable, Understandable, Robust
   - Color contrast requirements (4.5:1 for AA)
   - Keyboard accessibility (no traps, visible focus)
   - Screen reader testing (NVDA, JAWS, VoiceOver)
   - ARIA attributes and roles

3. **Usability Testing:**
   - User testing methodologies (moderated, unmoderated, guerrilla, remote)
   - Think-aloud protocol
   - Usability metrics (task success, time on task, error rate, satisfaction)
   - Nielsen's 10 Usability Heuristics
   - SUS and NPS scoring

4. **Compatibility Testing:**
   - Browser testing strategy (P0: Chrome, Safari)
   - CSS/JavaScript compatibility issues
   - Feature detection and fallbacks
   - Operating system differences (Windows, macOS, Linux, iOS, Android)
   - Cross-browser testing tools (BrowserStack, LambdaTest)

### Best Practices:

âœ“ Test with native speakers for localization
âœ“ Use pseudo-localization for testing i18n readiness
âœ“ Ensure WCAG Level AA compliance minimum
âœ“ Test with actual screen readers, not just automated tools
âœ“ Conduct usability tests with 5-8 users
âœ“ Prioritize browsers by market share
âœ“ Use feature detection, not browser detection
âœ“ Provide graceful degradation for unsupported features
âœ“ Document compatibility requirements clearly
âœ“ Test on real devices, not just emulators

### Tomorrow's Preview:

Day 12 will cover:
- Regression Testing strategies
- Building regression test suites
- Test case prioritization techniques
- Smoke testing in depth
- Optimization strategies for test execution
- Automation considerations for manual testers

---

**Congratulations on completing Day 11!**

**Study Time:** 5-6 hours

---

*End of Day 11 Study Guide*
